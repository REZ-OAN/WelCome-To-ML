{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBpd6AOxp+lTHi1AP1yFqp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/REZ-OAN/WelCone-To-ML/blob/main/TextProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Libraries That Will be used\n",
        "\n"
      ],
      "metadata": {
        "id": "YVRdVbojUi8b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkUAtpD9SlUL"
      },
      "outputs": [],
      "source": [
        "##stamming and lamitization\n",
        "from nltk.stem.porter import PorterStemmer as ps\n",
        "ss = ps()\n",
        "def stem_words(text):\n",
        "    return ' '.join([ss.stem(word) for word in text.split()])\n",
        "sample = \"walk walks walked walking\"\n",
        "tex = \" Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the , when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the  with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\"\n",
        "stem_words(tex)\n",
        "## english word hoy na root word ta dey\n",
        "## lamitization e english word e hoy (slow)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "wlr = WordNetLemmatizer()\n",
        "tex = \"Old education him departure any arranging one prevailed. Their end whole might began her. Behaved the comfort another fifteen eat. Partiality had his themselves ask pianoforte increasing discovered. So mr delay at since place whole above miles. He to observe conduct at detract because. Way ham unwilling not breakfast furniture explained perpetual. Or mr surrounded conviction so astonished literature. Songs to an blush woman be sorry young. We certain as removal attempt.\"\n",
        "punc = string.punctuation\n",
        "words = nltk.word_tokenize(tex)\n",
        "for word in words:\n",
        "    if word in punc:\n",
        "        words.remove(word)\n",
        "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
        "for word in words:\n",
        "    print(\"{0:20}{1:20}\".format(word,wlr.lemmatize(word,pos='v')))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zQi6FwDNQ4od"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}